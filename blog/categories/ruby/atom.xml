<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ruby | RJ's Notes]]></title>
  <link href="http://rurounijones.github.com/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://rurounijones.github.com/"/>
  <updated>2014-08-13T03:05:16+09:00</updated>
  <id>http://rurounijones.github.com/</id>
  <author>
    <name><![CDATA[RurouniJones]]></name>
    <email><![CDATA[jeff@jones.be]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bayesian Filter Performance in Ruby]]></title>
    <link href="http://rurounijones.github.com/blog/2014/08/11/bayesian-filter-performance-in-ruby/"/>
    <updated>2014-08-11T12:00:00+09:00</updated>
    <id>http://rurounijones.github.com/blog/2014/08/11/bayesian-filter-performance-in-ruby</id>
    <content type="html"><![CDATA[<p>A while ago I was in an online discussion regarding &ldquo;Correct Tools for
the Jobs&rdquo; and how ruby was not a good language for developing say, an
operating system.</p>

<p>Someone made a comment that it was not a good idea to use ruby for
bayesian filtering of things like forum posts. (Bayesian filtering is
one of the primary algorithms used for determining if an email or forum
post is spam or not).</p>

<p>They made some performance claims which seemeds exceedingly slow, but
also made some statements that made me suspect their application design
was not all that it could be so I thought I would see if ruby was the
guilty party.</p>

<p>This post is not primarily about bayesian filtering but about
performance testing; it is probably most helpful to low to intermediate
ruby developers.</p>

<!--more-->


<h2>TL;DR</h2>

<p>In this post I will write about:</p>

<ul>
<li>Investigating to see if ruby can bayesian filter 1000 posts / second</li>
<li>Finding a Bayesian filter</li>
<li>Finding an appropriate data-set</li>
<li>Writing some code to allow us to benchmark things</li>
<li>Identifying performance bottlenecks using the stackprof gem and
fixing them.</li>
<li>Great success!</li>
<li>&ldquo;I learned something today&rdquo;</li>
</ul>


<p>If you are here for the performance analysis stuff with Stackprof then
you can skip down to &ldquo;The Analysing&rdquo; section below.</p>

<h2>The Claim</h2>

<p>So first we need to identify the claims made against ruby, this will
determine what our goals are.</p>

<p>The poster made the following claims (Paraphrased slightly):</p>

<ul>
<li>&ldquo;Implemented a Bayesian Filter to help deal with forum spam&rdquo;</li>
<li>&ldquo;Tested under load&rdquo;</li>
<li>&ldquo;Sometimes took up to 15 seconds due to all the Array structures&rdquo;</li>
<li>&ldquo;The same implementation in C never exceeded half a second&rdquo;</li>
</ul>


<p>Now, if you are a ruby person the third claim might have been a red
flag for you (It was for me). &ldquo;All the Array Structures&rdquo;&hellip; Now when
it comes to searching, Arrays are slow, dog slow. A quick look at
our farvourite <a href="http://bigocheatsheet.com/">Big-O Notation Cheetsheet</a>
site shows that searching an array is an &ldquo;O*N&rdquo; operation. This means
that search time increases as the size of the array increases.</p>

<p>The other thing is that Arrays shouldn&rsquo;t really feature that much in
Bayesian filtering as the algorithm doesn&rsquo;t really care if a word
appears once or a hundred times.</p>

<p>These were the things that made me wonder if the poster was correct
in blaiming ruby for the slowness. I also happened to know that there
are some bayesian filter gems in the ruby-ecosystem which doesn&rsquo;t
really make sense if it is as slow as the poster claims.</p>

<p>When I pointed out the fact that it might be the poster&rsquo;s design they
didn&rsquo;t take it very well. They stated the challenge to create a userland
ruby application that processes 1000 unique variable length posts a
second. Their training data set was 100,000 posts.</p>

<h2>The Goal</h2>

<p>So, now we know the claims we need the goal.</p>

<ul>
<li>Benchmark a ruby Bayesian filter.</li>
<li>Try and attain a filtering rate in the neighbourhood of 1000 posts
/ second. Say +- 10%</li>
<li>Do the above with a training data-set of around 100,000 items.</li>
</ul>


<h2>The Filter</h2>

<p>Right sports fans! The first thing we need to do is write a bayesian
filter in ruby&hellip; Bwahahahahah, I make myself laugh. I am a great
believer in standing on the shoulders of giants so instead of writing
a filter lets find some likely ruby gems.</p>

<p>Some searching on github and &ldquo;The Ruby Toolbox&rdquo; lead me to the
<a href="https://github.com/cardmagic/classifier">Classifier</a> gem, however
the last commit was a bit old and there was
<a href="https://github.com/cardmagic/classifier/issues/4">this</a> issue which
called Classifier&rsquo;s accuracy into question which, at the time, was
not fixed.</p>

<p>Now, at this point I should confess, I only have a vague overview
level of knowledge on how bayesian filtering works and I was hoping
to spare my brain-power by not learning the nitty gritty mathematical
details (One of the reasons I was looking for a Gem in the first place)</p>

<p>However, the poster of that issue wrote their own gem instead called
<a href="https://github.com/bmuller/ankusa">Ankusa</a> which looks pretty good.
So lets go with that.</p>

<h2>The Dataset</h2>

<p>So now we have our filter we just need a data-set. The original poster
said that they used a training data set of 100,000 posts. I am going
to assume a roughtly 50/50 split here and say they had 50,000 known
good posts (from now on called &lsquo;ham&rsquo;) and 50,000 spam posts.</p>

<p>After some <a href="https://duckduckgo.com">DuckDuckGo</a>&lsquo;ing I found some
likely sources of curated spam/ham data-sets. It looked like at least
one good thing came out of the
<a href="http://en.wikipedia.org/wiki/Enron_scandal">Enron Collapse</a> which is
that their emails were made public as part of the discovery process.</p>

<p>Some researchers then put them up for
<a href="http://www.cs.cmu.edu/~enron/">download</a>. While the numbers aren&rsquo;t
quite up to the 100,000 posts the original poster said they are
in the same ballpark.</p>

<ul>
<li>19,089 ham emails</li>
<li>32,989 spam emails</li>
</ul>


<p>For a grand-total of around 50,000 emails. Due to the way the filter
is written I don&rsquo;t see there being much difference in speed between
50,000 and 100,000 items in the training set.</p>

<p>NOTE: If you are reading this and know of a bigger training set that
is freely available I would love to hear about it.</p>

<p>Wow, 750 odd words into this article and we haven&rsquo;t even gotten to the
beginning of the code stuff yet.</p>

<h2>Assumptions</h2>

<p>Before I continue with the actual code stuff I am also going to have
to make some assumptions on how the filter was actually used. (At least
this is probably how I would have done it) I hope you will agree that
these are logical assumptions:</p>

<ul>
<li><p>A dedicated Bayesian machine: If you are processing 1000 forum
posts / second I am going to assume that your operation is large enough
to warrent a dedicated bayesian filtering server (Probably at the end of
a REST API or message queue called by your front-end machines)</p></li>
<li><p>No online updating of the training data-set: I will assume the training
set is batch-updated at some point (Maybe a daily / weekly thing).</p></li>
<li><p>The training data-set is held in memory: After implementating the
training code using the Enron data-set I found that the training data only
took up about 17MB of disk-space. Since this is pretty small and we assumed
we have a dedicated server (See Assumption #1) that we will hold it in
memory for maximum performance rather than a database.</p></li>
</ul>


<h2>The Code</h2>

<p>Before we can benchmark Ankusa we need some sort of test-runner code.
To that end I present to you
<a href="https://github.com/rurounijones/dont_bayes_me_bro">&ldquo;Don&rsquo;t Bayes Me Bro&rdquo;</a>
(DBMB: Well it made me chuckle when I named it and I like the idea of
spammers being tazered).</p>

<p>All code samples from here-on-out will come from either Ankusa or DBMB.</p>

<p>WARNING: DBMB code is messy and not TDD&rsquo;d and likely to make your
&ldquo;Beautiful Code&rdquo; gland rupture.</p>

<h3>Training</h3>

<p>The first thing we need to do is create our training data-set.</p>

<p>The actual code is <a href="https://github.com/rurounijones/dont_bayes_me_bro/blob/master/lib/dont_bayes_me_bro/trainer.rb">here</a></p>

<p>DBMB has a &ldquo;training&rdquo; folder into which we dumped the Enron emails from
earlier. All we are doing in this code recursively reading files from
the &ldquo;spam&rdquo; and &ldquo;ham&rdquo; sub-directories and training ankusa with them.</p>

<p>We then save the data to a file called &ldquo;corpus&rdquo;. Note that since we
are doing an operation that uses file I/O we can speed things up by
creating one thread for &ldquo;spam&rdquo; and one for &ldquo;ham&rdquo;. The GIL gets in the
way a bit but still get a performance speed-up in this case.</p>

<p>Since the original poster was talking about &ldquo;Forum Threads&rdquo; I decided
to just parse the email body rather than everything including headers
etc.</p>

<p>This is kicked off from a Rake task.</p>

<h3>Benchmarking</h3>

<p>Benchmarking is done <a href="https://github.com/rurounijones/dont_bayes_me_bro/blob/master/lib/dont_bayes_me_bro/benchmarker.rb">here</a>
and is started from another Rake task which allows us to test 1,000
to 30,000 emails using the original data-set from which the corpus
came from.</p>

<p>To run the benchmark we insert the required number of email bodies
into a queue. We then pop from the queue and run the filter. Why a queue?
Well I was thinking down the road when multi-threading might make
an appearance.</p>

<p>A few things to note. To avoid skewing the benchmarks we pre-initialize
a two variables which are otherwise lazy loaded by Ankusa.</p>

<p>To keep things deterministic we save our queue data to a file for
future reading, this also saves time over parsing thousands of emails
each time. We also avoid emails with VERY short bodies (Less than 100
characters) to avoid getting an corpus with an overly short average
body length.</p>

<h2>The Analysis</h2>

<p>Ok, here it is, the moment we have all been waiting for. After lots
of waffle about setting this up, it is time for the main event, the
actual benchmarking, the thing you are actually reading this blog post
about probably, the thing that is at the end of this overly long sentence
which is driving you insane!</p>

<p>Note: In the interest of brevity (In a post this long?! HAH) I have
removed a lot of unnecessary output from the commands.</p>

<p>Test Machine:</p>

<ul>
<li>CPU:  Intel&reg; Core&trade; i7-2600 CPU @ 3.40GHz</li>
<li>OS:   Kubuntu 14.04</li>
<li>Ruby: 2.1.2</li>
</ul>


<pre><code>$ rake benchmark:1000

Benchmarking 1000 emails
Starting benchmark
                 user     system      total        real
1000         6.260000   0.010000   6.270000 (  6.276492)
Jobs per second: 159
</code></pre>

<p>Oook then, roughly 159 jobs per second. 1/10th of what we need. This is
not the end of the world but it was a bit slower than I was hoping for.</p>

<p>So, first step is to analyse the code. For this we will use the godly,
amazing, idiot-proof <a href="https://github.com/tmm1/stackprof">Stackprof</a> gem
by tmm1. Stackprof is introduced by tmm1
<a href="http://tmm1.net/ruby21-profiling/">here</a> and I highly recommend reading
it.</p>

<p>So let us run the test again with stackprof enabled and see what we can
see.</p>

<pre><code class="bash">$ PROFILE=1 rake benchmark:1000

Benchmarking 1000 emails
Starting benchmark
                 user     system      total        real
1000         6.570000   0.020000   6.590000 (  6.593799)
Jobs per second: 151

$ stackprof /tmp/dbmb-20140811050715.dump --limit 5

==================================
  Mode: cpu(1000)
  Samples: 1597 (3.04% miss rate)
  GC: 101 (6.32%)
==================================
     TOTAL    (pct)     SAMPLES    (pct)     FRAME
      1321  (82.7%)        1137  (71.2%)     Ankusa::TextHash.valid_word?
       184  (11.5%)         111   (7.0%)     String#numeric?
        73   (4.6%)          73   (4.6%)     rescue in String#numeric?
        32   (2.0%)          32   (2.0%)     String#stem
        62   (3.9%)          30   (1.9%)     Ankusa::TextHash#add_word
</code></pre>

<p>Here I have told stackprof to identify the methods which were taking up
the most runtime and limit it to the longest 4 methods. And holy moly!</p>

<p>From reading line 17 we can tell that the vast VAST VAST majority of
the runtime is taken up with a single method! This is good and bad.
Good, in that if we can optimise this then it will be a huge win, bad in
that if we cannot, we are screwed. So let us look at the offending code.</p>

<pre><code class="ruby"># word should be only alphanum chars at this point
def self.valid_word?(word)
  not (Ankusa::STOPWORDS.include?(word) || word.length &lt; 3 || word.numeric?)
end
</code></pre>

<p><a href="https://github.com/bmuller/ankusa/blob/782dba5601eafe83890faaae932177769eb5ffb7/lib/ankusa/hasher.rb#L20">The full code</a></p>

<p>Hunh, ok, not that complicated. But the first part of this stands out.
<code>.include?</code> is used on enumerables. Enumerables like&hellip;. Arrays? Let us
check.</p>

<pre><code class="ruby">
module Ankusa
  STOPWORDS = %W(lots-of-words)
end
</code></pre>

<p><a href="https://github.com/bmuller/ankusa/blob/782dba5601eafe83890faaae932177769eb5ffb7/lib/ankusa/stopwords.rb">The full code</a></p>

<p>I have removed all the words from STOPWORDS but let me tell you it
had 544 entries. So what we have here is a 544 entry Array that is
searched for every.. SINGLE&hellip; WORD! Remember what we said about
searching arrays? O*N average complexity, as the size of the array
increases so does the time it takes to search. We can show this using
a micro-benchmark.</p>

<p>The following code searches a set of arrays 5000 times.</p>

<pre><code class="bash">                                     user     system      total        real
one_word_array.include?          0.020000   0.000000   0.020000 (  0.018448)
ten_word_array.include?          0.030000   0.000000   0.030000 (  0.027409)
twenty_word_array.include?       0.040000   0.000000   0.040000 (  0.046323)
hundred_word_array.include?      0.180000   0.000000   0.180000 (  0.179623)
five_hundred_word_array.include? 0.320000   0.000000   0.320000 (  0.319819)
</code></pre>

<p>You can see how the time take to run <code>.include?</code> increases with the
size of the array.</p>

<p>So what are we to doooooo!? Well, when we have an array for the
sole purpose of calling <code>include?</code> on it we do not care about
duplicate values. Therefore we can use another, underused Ruby
Data-structure.</p>

<p>Tadaaaa! <a href="http://www.ruby-doc.org/stdlib-2.1.2/libdoc/set/rdoc/Set.html">Set</a>
to the rescue! Sets are similar to Arrays with a few key differences.</p>

<p>But the big one, the BIG one, is that, unlike Arrays, Sets use the
same &ldquo;Hash Table&rdquo; data-structure as ruby Hashes to store their data.
What does this mean? Well another visit to
<a href="http://bigocheatsheet.com/">Big-O Notation Cheetsheet</a> tells us that
Hash Tables are much MUCH better for searching with an average complexity
of O*1 and a worst-case of O*N.</p>

<p>This means that, even as the size increases the search-time remains
relatively constant. Let us test this again with our micro-benchmark.</p>

<pre><code class="bash">                                     user     system      total        real
one_word_set.include?            0.010000   0.000000   0.010000 (  0.010633)
ten_word_set.include?            0.020000   0.000000   0.020000 (  0.012516)
twenty_word_set.include?         0.010000   0.000000   0.010000 (  0.010707)
hundred_word_set.include?        0.010000   0.000000   0.010000 (  0.013273)
five_hundred_word_set.include?   0.010000   0.000000   0.010000 (  0.013702)
</code></pre>

<p>Sweeeeeeeeeet. So let us replace the Ankusa::STOPWORDS Array with a Set</p>

<pre><code class="ruby">require 'set'

module Ankusa
  STOPWORDS = Set.new %W(lots-of-words)
end
</code></pre>

<p>and see what happens.</p>

<pre><code class="bash">                 user     system      total        real
1000         1.950000   0.010000   1.960000 (  1.966099)

Jobs per second: 508
</code></pre>

<p>Wow, it just goes to show how a tiny, simple change can have such a huge
impact. A running time reduced from about 6 seconds to 2 seconds, we are
now halfway to our goal!</p>

<p>However halfway is not all the way. Improving performance is a simple cycle</p>

<ul>
<li>Benchmark</li>
<li>Find bottleneck</li>
<li>Fix bottleneck</li>
<li>Start again</li>
</ul>


<p>We have fixed out first bottleneck, let us find the next one using trusty
stackprof again.</p>

<pre><code class="bash">==================================
  Mode: cpu(1000)
  Samples: 498 (9.12% miss rate)
  GC: 102 (20.48%)
==================================
     TOTAL    (pct)     SAMPLES    (pct)     FRAME
       179  (35.9%)         105  (21.1%)     String#numeric?
        74  (14.9%)          74  (14.9%)     rescue in String#numeric?
        45   (9.0%)          44   (8.8%)     Ankusa::TextHash.atomize
        57  (11.4%)          40   (8.0%)     Ankusa::TextHash#add_word
        24   (4.8%)          24   (4.8%)     Set#include?
</code></pre>

<p>So the next longest action is actually a method on String which is checking
if the string is numeric or not and it there is some rescue action going on
in there which means it is taking up the top two spots. Now String is a
ruby core class and it does not have a <code>numeric?</code> method by default so
this looks like something Ankusa has monkey-patched in.</p>

<p>A quick look through the source-code and we see this is the case (in the
appropriately named <code>extensions.rb</code>)</p>

<pre><code class="ruby">class String
  def numeric?
    true if Float(self) rescue false
  end
end
</code></pre>

<p><a href="https://github.com/bmuller/ankusa/blob/8b223a07a85847abb1ec76ec6746face3a388635/lib/ankusa/extensions.rb">The full code</a></p>

<p>So what is wrong with this method? Well, nothing is wrong with it per se,
it is one of the standard ways to see if a String is numeric or not.</p>

<p>The problem with it is that it is slow, it is even slower if the string
is not numeric because then it raises an exception which has to be rescued
(sloooooooooooow). This is one of the reasons you see people saying.
&ldquo;Don&rsquo;t use exceptions for flow-control!&rdquo;.</p>

<p>The problem is that we cannot really change this because every other way
of checking if a string is numeric or not has edge-cases where they fail.
This is the only bullet-proof way of making sure if a String is numeric
or not.</p>

<p>But do we need bullet-proofness? Lets have a nose around and see if there
is any other option.</p>

<p>If we go up the call chain a bit we can see that each word is processesed
in add_text:</p>

<pre><code class="ruby">def add_text(text)
  if text.instance_of? Array
    text.each { |t| add_text t }
  else
    # replace dashes with spaces, then get rid of non-word/non-space characters,
    # then split by space to get words
    words = TextHash.atomize text
    words.each { |word| add_word(word) if TextHash.valid_word?(word) }
  end
  self
end
</code></pre>

<p>To create a &ldquo;word&rdquo; it first atomises any text passed to it. The comment
looks very interesting &ldquo;Replace dashes with spaces&rdquo;&hellip; well that would
remove negative numbers for starters. Lets have a look at the <code>atomize</code>
method.</p>

<pre><code class="ruby">def self.atomize(text)
  text.downcase.to_ascii.tr('-', ' ').gsub(/[^\w\s]/," ").split
end
</code></pre>

<p>Hmm, this looks interesting, this code basically strips all dashes and
replaces anything that is not a word or whitespace character with a space.
Lets assume our regexp knowledge is fuzzy and we are not sure what a
&ldquo;word&rdquo; (\w) is, we can fire up IRB and do some testing:</p>

<pre><code class="ruby">$ irb
2.1.2 :001 &gt; # Include the monkey-patch required by atomise
2.1.2 :002 &gt;   class String
2.1.2 :003?&gt;     def to_ascii
2.1.2 :004?&gt;         encode("UTF-8", :invalid =&gt; :replace, :undef =&gt; :replace, :replace =&gt; "").force_encoding('UTF-8') rescue ""
2.1.2 :005?&gt;       end
2.1.2 :006?&gt;   end
 =&gt; :to_ascii
2.1.2 :007 &gt;
2.1.2 :008 &gt;   def atomize(text)
2.1.2 :009?&gt;     text.downcase.to_ascii.tr('-', ' ').gsub(/[^\w\s]/," ").split
2.1.2 :010?&gt;   end
 =&gt; :atomize
2.1.2 :011 &gt;
2.1.2 :012 &gt;   atomize("1234")
 =&gt; ["1234"]
2.1.2 :013 &gt; atomize("-1234")
 =&gt; ["1234"]
2.1.2 :014 &gt; atomize("-1234.56")
 =&gt; ["1234", "56"]
</code></pre>

<p>Well, with some experimentation it looks like any kind of number will
always be split into a bunch of integers. This means that we don&rsquo;t
really need the edge-case surety of <code>Float(string)</code>. Lets see how much
faster a simple regex is.</p>

<pre><code class="ruby">class String
  def numeric?
    word.match(/[\d+]/)
  end
end
</code></pre>

<p>And the result:</p>

<pre><code class="bash">                 user     system      total        real
1000         1.050000   0.000000   1.050000 (  1.052543)

Jobs per second: 950
</code></pre>

<p>Great success! Another simple change, another massive speed-up.</p>

<p>Eagle-eye mathematicians might notice that we DO have an edge-case
that we are no longer covering in that numbers like 1.05e16 will
end up as <code>["1","05e16"]</code>. However by the time we check if a String
is numeric this number has already been mashed up and checking for
<code>[\d.]+(?:e?\d+)</code> could result in us ignoring words that we would
prefer to check. All in all I think it is safer to not ignore a
string like &ldquo;1e05&rdquo;.</p>

<p>Others may cringe at having such a method as a monkey-patch on
String but do not worry, in the real PR I also moved it out of there
as evidenced <a href="https://github.com/bmuller/ankusa/commit/2d3d916640937680b3653db781357707b669be3d">here</a></p>

<h2>Great Success</h2>

<p>We are now close enough to 1000 jobs per second that I am going to call time on this
post. There are other optimisations we could probably do but we have already
done the easy stuff as evidenced by stackprof once again.</p>

<pre><code class="bash">==================================
  Mode: cpu(1000)
  Samples: 210 (20.15% miss rate)
  GC: 0 (0.00%)
==================================
     TOTAL    (pct)     SAMPLES    (pct)     FRAME
        48  (22.9%)          44  (21.0%)     Ankusa::TextHash.atomize
        27  (12.9%)          27  (12.9%)     Ankusa::TextHash.numeric_word?
        45  (21.4%)          26  (12.4%)     Ankusa::TextHash#add_word
        23  (11.0%)          23  (11.0%)     Set#include?
        19   (9.0%)          19   (9.0%)     String#stem
        14   (6.7%)          14   (6.7%)     Ankusa::MemoryStorage#get_word_counts
        13   (6.2%)          13   (6.2%)     block (2 levels) in Ankusa::NaiveBayesClassifier#log_likelihoods
        13   (6.2%)          13   (6.2%)     block in Ankusa::Classifier#get_word_probs
        48  (22.9%)          10   (4.8%)     Ankusa::Classifier#get_word_probs
         8   (3.8%)           5   (2.4%)     block in Ankusa::Classifier#get_word_probs
         4   (1.9%)           4   (1.9%)     String#to_ascii
         3   (1.4%)           3   (1.4%)     Ankusa::Classifier#vocab_sizes
         3   (1.4%)           3   (1.4%)     Ankusa::MemoryStorage#get_total_word_count
       210 (100.0%)           2   (1.0%)     Ankusa::NaiveBayesClassifier#classify
         2   (1.0%)           1   (0.5%)     block in Ankusa::NaiveBayesClassifier#log_likelihoods
         1   (0.5%)           1   (0.5%)     Ankusa::Classifier#doc_count_totals
         1   (0.5%)           1   (0.5%)     Ankusa::MemoryStorage#get_doc_count
         1   (0.5%)           1   (0.5%)     block in Ankusa::NaiveBayesClassifier#classify
</code></pre>

<p>As we can see from this (full) output there is no horrendously slow
bottleneck that we can fix for a big win.</p>

<p>(A cofession here: I got the &ldquo;Need for Speed&rdquo; bug at this point and did
some more tweaking that got us to about 970-980 jobs per second, you
can see the full list of changes <a href="https://github.com/rurounijones/ankusa/commits/performance-increases">here</a></p>

<h2>Where Next?</h2>

<p>So we had a challenge and I think we met it, We didn&rsquo;t do much in the
way of long-run tests and our setup might have differed from theirs but
I think I showed that ruby can have respectable results. This means that
Ruby is perfect, right?.</p>

<p>Well no, while I do believe the original poster was wrong to blame
ruby for his application&rsquo;s slowness there are a few issues here.</p>

<p>First is that using ankusa in this manner is massively CPU bound we are
stuck here using a single thread. This operation would benefit hugely from
effective multi-threading but Ruby&rsquo;s GIL prevents us from doing so since we are
not doing much in the way of I/O.</p>

<p>JRuby to the rescue! I did actually try testing on JRuby. Ankusa
actually uses a C Extension for the word-stemming and there is a JRuby
drop-in equivalent but when I ran the tests on JRuby it was
horrendously slow (Something like 40 times slower) and at that point
I was not really up for trying to figure out why.</p>

<p>There is always Rubinius, I have never used it to be honest, but it
does sound ideal for this case, maybe I will write a part 2 (RBX Redux!).</p>

<h2>I Learned Something Today</h2>

<p>What I hoped I demonstrated was that improving performance is not
the black-magic beginners might think it is. There are tools that make
it dead simple to do so I highly recommend you give it a go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Resources for Ruby Developers]]></title>
    <link href="http://rurounijones.github.com/blog/2013/03/04/resources-for-ruby-developers/"/>
    <updated>2013-03-04T12:40:00+09:00</updated>
    <id>http://rurounijones.github.com/blog/2013/03/04/resources-for-ruby-developers</id>
    <content type="html"><![CDATA[<p>As someone (Probably a really famous guy whose name I cannot remember) once said
becoming a software developer means having to learn new things for the rest of
your life.</p>

<p>This is just as true for Ruby as for any other language so here are the
resources I like to use to keep up to date.</p>

<p>These are besides things like API sites and the ruby on rails guides etc.</p>

<!--more-->


<h2><a href="http://railscasts.com">Railscasts</a></h2>

<p>Made by Ryan Biggs, Railscasts is a great site with webcasts exploring new and
interesting things happening in the ruby world. As the name suggests it is more
focused on Web Development and Ruby on Rails but it also explores technology
that can be useful to other types of ruby developers.</p>

<p>There are free and paid-for episodes available (for US$9 per month) and every
episode has source code avilable on github. Most episodes also have full
transcripts, including code, which can be read if you are not a video-watching
type.</p>

<h2><a href="http://confreaks.com">Confreaks</a></h2>

<p>If you do not have money to travel the world attending conferences and your
company does not stump up the cash then Confreaks is a gem of a site. They
record presentations at a lot of major development conferences.</p>

<p>Confreaks is very popular with ruby conferences so you can find a wealth of
interesting talks there. (Most of the talks are about 40-60 minutes long
which is perferct for filling up a lunch break while eating or watching on the
daily commute)</p>

<h2><a href="http://news.ycombinator.com">Hacker News</a></h2>

<p>Hacker News is a (mostly) news aggregation site that has reasonable high
standards of submissions and comments. Like all aggregation sites it has plenty
of stories that do not interest me directly but there is a lot of good technical
news / startup news there.</p>

<p>Requires a bit more mental filtering but still worth it. Just be aware that it
is quite start-up focused which can cause a bit of an echo chamber effect. A
healthy dose of cynicism (realism?) is useful.</p>

<h2><a href="http://reddit.com">Reddit</a></h2>

<p>Ah, Reddit, the &ldquo;Front page of the internet&rdquo; and a hell of a good time-waster.
Beware of this site in that regard.</p>

<p>Reddit has various subforums related to specialised topics. For the Ruby
developer about town the following Reddits may prove useful:</p>

<ul>
<li><a href="http://reddit.com/r/ruby">Ruby</a></li>
<li><a href="http://reddit.com/r/rubyonrails">Ruby on Rails</a></li>
<li><a href="http://reddit.com/r/webdev">Webdev</a></li>
</ul>


<h2><a href="http://ruby5.envylabs.com">Ruby5</a></h2>

<p>Ruby5 is a podcast from the gents at Envy Labs which covers interesting ruby
related things. They are relatively short and to the point (About 5 minutes)</p>

<p>To be honest, I do not listen to the podcast that often but they list each tech
they talk about on the page for that episode which is what I usually have a look
at.</p>

<h2><a href="http://rubyshow.com">RubyShow</a></h2>

<p>In the same manner as Ruby5 above this podcast talks about interesting things
going on in the ruby world. The podcasts from Rubyshow are a lot longer than
ruby5.</p>

<p>Again like Ruby5 I mainly use this site for the list of technologies they have
in the transcript rather than listening to the podcast itself. (Aren&rsquo;t I an
old fashioned duffer&hellip;)</p>

<h2><a href="http://webpulp.tv">Webpulp</a></h2>

<p>While I am mainly a ruby dev I am also resposible for other aspects such as
server administration etc. (Not my main job but hey).</p>

<p>Webpulp.tv is basically a series of interviews with employees of quite famous
tech related companies.</p>

<p>The interviews usually focus on the technology used by the company, challenges
they have faced and how they got around them etc.</p>

<p>It is exceedingly useful to see what others are using in their stacks and a good
way to learn about new and interesting software / techniques that might be
useful for you.</p>

<p>Unfortunately webpulp does not update very often but what the hey.</p>

<h2>Company blogs</h2>

<p>As well as the above I have blogs by various ruby related companes on an RSS
feed. for example:</p>

<ul>
<li><a href="http://37signals.com/svn">37Signals</a></li>
<li><a href="http://www.engineyard.com/blog">EngineYard</a></li>
<li><a href="http://robots.thoughtbot.com/">Thoughtbot</a></li>
</ul>


<p>You should find companies doing stuff that interest you and sign up to their
blogs</p>

<h2><a href="http://rubular.com/">Rubular</a></h2>

<p>This is a handy little site which lets you write and test regular expressions
as you write them. It is my go-to site when I need to use regexes (which is not
that common, hence why it is such a useful site)</p>

<h2>Other things</h2>

<p>If you have read this far then I thank you for your attention and would like to
use it to remind you of one thing.</p>

<p>Everything above is optional. What you should be doing anyway is being signed up
to the security mailing lists of all the major components in your stack.</p>

<p>For example if you you Ruby on Rails with PostgreSQL and Redis datastores and a
Varnish cache running on CentOS then you should be signed up to the security
mailing lists of all of these.</p>

<h2>A Final Warning / Bit of Encouragement</h2>

<p>I have listed a reasonable number of resources above. This list is no-where near
exhaustive and you should be building up your own list of go-to resources. As
well as this you need to beware of something I call &ldquo;Learner&rsquo;s Paralysis&rdquo;.</p>

<p>Following all of the above sites (plus ones you find yourself) can take up a
significant chunk of your time if you let it.</p>

<p>Do not let it take up so much that you end up reading / learning a lot more than
doing. This is a problem I suffer from, I find learning about these things so
interesting (at a superficial level) that I don&rsquo;t actually get round to <em>doing</em>
anything. (Work on side-projects, think about that bootstrapped business
I want to to start etc.)</p>

<p>Get out there and put the stuff to use rather than just thinking &ldquo;Hey, I learned
something&rdquo; and leaving it at that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Textile filtering with RedCloth]]></title>
    <link href="http://rurounijones.github.com/blog/2009/10/24/textile-filtering-with-redcloth/"/>
    <updated>2009-10-24T18:00:00+09:00</updated>
    <id>http://rurounijones.github.com/blog/2009/10/24/textile-filtering-with-redcloth</id>
    <content type="html"><![CDATA[<p>I wanted to use RedCloth to let users of my rails app make textile formatted
posts but I wanted to restrict the input they were allowed to use. How to do
this? I thought it would be simple.</p>

<p><strong>Warning: This article was imported from an old site and is therefore itself
rather old. It may not still be accurate for current versions of RedCloth.</strong></p>

<!--more-->


<p>Textile has a +filter_html+ option which I thought would do the trick but that
only filters what HTML RedCloth allows users to enter. It doesn’t filter the
HTML created by Redcloth itself when a user uses textile tags.</p>

<p>So how to filter the textile tags?</p>

<p>First, assuming you are using Rails 2.3 or later create the following file.
For other frameworks please use the recommended method for adding start-up
code to that framework.</p>

<pre><code>config/initializers/redcloth_filter.rb
</code></pre>

<p>This file will be run during the rails initialization and will contain the code
we want to override (monkey-patch). Now paste the following code into the file.</p>

<pre><code class="ruby config/initializers/redcloth_filter.rb">module RedCloth::Formatters::HTML
  include RedCloth::Formatters::Base

  ALLOWED_TAGS = {
      'a' =&gt; ['href', 'title'],
      'br' =&gt; [],
      'i' =&gt; nil,
      'u' =&gt; nil,
      'b' =&gt; nil,
      'pre' =&gt; nil,
      'kbd' =&gt; nil,
      'code' =&gt; ['lang'],
      'cite' =&gt; nil,
      'strong' =&gt; nil,
      'em' =&gt; nil,
      'ins' =&gt; nil,
      'sup' =&gt; nil,
      'sub' =&gt; nil,
      'del' =&gt; nil,
      'table' =&gt; nil,
      'tr' =&gt; nil,
      'td' =&gt; ['colspan', 'rowspan'],
      'th' =&gt; nil,
      'ol' =&gt; ['start'],
      'ul' =&gt; nil,
      'li' =&gt; nil,
      'p' =&gt; nil,
      'h3' =&gt; nil,
      'h4' =&gt; nil,
      'h5' =&gt; nil,
      'h6' =&gt; nil,
      'blockquote' =&gt; ['cite'],
  }

  def after_transform(text)
    text.chomp!
    clean_html(text, ALLOWED_TAGS)
  end    
end
</code></pre>

<p>ALLOWED_TAGS is a hash of tags that you want to allow. You can take the
<a href="http://redcloth.rubyforge.org/classes/RedCloth/Formatters/HTML.html">BASIC_TAGS</a>
to use as a base and strip tags you don’t want to allow from the hash and add
other ones if you want to.</p>

<p>So we have defined the tags that we want to allow. Now we need to actually do
some stripping. This is where the after_transform method comes in. This is
called by RedCloth as standard after initial modification. So what we can do is
override the method and tell RedCloth to clean_html again with the HTML string
it has just created. To give you a list of steps.</p>

<ul>
<li>RedCloth is configured with +filter_html+ enabled</li>
<li>User enters string (Textile and HTML)</li>
<li>RedCloth strips HTML tags from the string according to the BASIC_TAGS using
clean_html method</li>
<li>RedCloth converts the textile tags in the string to HTML</li>
</ul>


<p>At this point the HTML’ised string is usually returned; however we do some
overriding so that:</p>

<ul>
<li>RedCloth strips HTML tags from the above generated HTML string according to
our ALLOWED_TAGS using the clean_html method</li>
<li>RedCloth returns the twice filtered HTML string.</li>
</ul>


<p>Thinking about it you don’t even need +filter_html+ since it will all be
filtered the second time around explicitly by our code. However I feel a little
more secure by stripping all the user generated HTML cruft first using
+filter_html+ before stripping our textile generated HTML ourselves.</p>

<p>Enjoy</p>
]]></content>
  </entry>
  
</feed>
